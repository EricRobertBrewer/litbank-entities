{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e01e402-3697-4434-8b07-03e979eec055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/brewer/Code/UU/CS6390/litbank-entities\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1443baf-4958-4e74-bf60-10a8b730dc75",
   "metadata": {},
   "source": [
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c95e18b-17e5-43e8-83b0-4752d26ed152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 13:20:41.614170: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/brewer/Code/UU/CS6390/litbank-entities/venv/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import os\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from litbank_entities import extract, linguistics, litbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "720f87f6-052d-4cf1-8dd4-a7d664883aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = 'models'\n",
    "MODEL_NAME = 'demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a9add-669b-4035-b6c1-36bc16c8f19e",
   "metadata": {},
   "source": [
    "## Load data, language utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9443c036-2025-4cfc-887d-0d7a6e648598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences: 8567\n"
     ]
    }
   ],
   "source": [
    "text_sentence_tokens, text_sentence_labels = litbank.get_text_sentence_tokens_labels()\n",
    "sentence_tokens, sentence_labels = litbank.flatten_texts(text_sentence_tokens, text_sentence_labels)\n",
    "sentence_tokens, sentence_labels = linguistics.process(sentence_tokens, sentence_labels)\n",
    "sentence_tokens, sentence_labels = litbank.split_large_sentences(sentence_tokens, sentence_labels)\n",
    "assert len(sentence_tokens) == len(sentence_labels)\n",
    "print('Train sentences: {:d}'.format(len(sentence_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ddca3c1-c156-45e1-b29d-4755e732c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = linguistics.get_nlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de3c2f-0ad6-4528-8bb3-f2b804642bce",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835896a4-d1f2-43e1-be99-f00be1adae0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac9a0f6a23e448d844ad0b5a5d9293b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', layout=Layout(width='36em'), options=('DistilBERT (Transformer-based, condensed…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_options = [\n",
    "    'DistilBERT (Transformer-based, condensed)',\n",
    "    'CRF (Conditional Random Field)',\n",
    "    'HMM (Hidden Markov Model using only tokens)',\n",
    "    'ZeroR (dummy baseline)'\n",
    "]\n",
    "model_names = ['bert', 'crf', 'hmm', 'zero']\n",
    "model_option_to_name = {model_options[i]: model_names[i] for i in range(len(model_options))}\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_options,\n",
    "    value=model_options[0],\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    "    layout={'width': '36em'},\n",
    ")\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034a710d-d44c-4f87-9fee-100d4261bdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForTokenClassification.\n",
      "\n",
      "All the layers of TFDistilBertForTokenClassification were initialized from the model checkpoint at models/bert/demo/model_PER.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model.\n"
     ]
    }
   ],
   "source": [
    "classname = model_option_to_name[model_dropdown.value]\n",
    "categories = ('PER',)  # litbank.ENTITY_CATEGORIES\n",
    "resources = extract.create_model_resources(classname)\n",
    "model = extract.create_model(classname, categories, resources)\n",
    "\n",
    "model_dir = os.path.join(MODELS_DIR, classname)\n",
    "demo_dir = os.path.join(model_dir, MODEL_NAME)\n",
    "os.makedirs(demo_dir, exist_ok=True)\n",
    "if not os.path.exists(demo_dir) or \\\n",
    "        any(not os.path.exists(os.path.join(demo_dir, 'model_{}'.format(category))) for category in categories):\n",
    "    print('Training model...')\n",
    "    model.train(sentence_tokens, sentence_labels)\n",
    "    print('Done.')\n",
    "    model.save_model(demo_dir)\n",
    "    print('Saved model.')\n",
    "else:\n",
    "    model.load_model(demo_dir)\n",
    "    print('Loaded model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055881a3-a566-4542-b391-bb435e281088",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f08c28e-1e82-4700-a706-122ee6cb2fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3185abb5f7f4f2f8f612fe96f4a4a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='We have witnessed this morning the distant view a brief full battle of Pearl Harbor and the se…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pearl_harbor = \\\n",
    "    'We have witnessed this morning the distant view a brief full battle of Pearl Harbor' \\\n",
    "    ' and the severe bombing of Pearl Harbor by enemy planes, undoubtedly Japanese.\\n' \\\n",
    "    'The city of Honolulu has also been attacked and considerable damage done.'\n",
    "sentence_textarea = widgets.Textarea(\n",
    "    value=pearl_harbor,\n",
    "    placeholder='Sentences separated by newline characters.',\n",
    "    description='Find entities:',\n",
    "    layout={'height': '18em', 'width': '60em'},\n",
    ")\n",
    "display(sentence_textarea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e67dbca-7a02-4c75-91fe-3b639834db07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 13:20:58.743233: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [2,31]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 4s 45ms/step\n"
     ]
    }
   ],
   "source": [
    "test_sentence_tokens = [list(map(str, nlp(sentence))) for sentence in sentence_textarea.value.split('\\n')]\n",
    "test_sentence_preds = model.predict(test_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7dc1ec-1d3d-4b4e-b835-e09f03f5b005",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) ['We', 'have', 'witnessed', 'this', 'morning', 'the', 'distant', 'view', 'a', 'brief', 'full', 'battle', 'of', 'Pearl', 'Harbor', 'and', 'the', 'severe', 'bombing', 'of', 'Pearl', 'Harbor', 'by', 'enemy', 'planes,', 'undoubtedly', 'Japanese.']\n",
      "\n",
      "    Phrases (PER):\n",
      "\n",
      "\n",
      "(2) ['The', 'city', 'of', 'Honolulu', 'has', 'also', 'been', 'attacked', 'and', 'considerable', 'damage', 'done.']\n",
      "\n",
      "    Phrases (PER):\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category_test_sentence_phrases = litbank.get_category_sentence_phrases(test_sentence_preds, categories=categories)\n",
    "for i, preds in enumerate(test_sentence_preds):\n",
    "    tokens = test_sentence_tokens[i]\n",
    "    print('({:d}) {}'.format(i + 1, tokens))\n",
    "    print()\n",
    "    for k, category in enumerate(categories):\n",
    "        print('    Phrases ({}):'.format(category))\n",
    "        for j, phrase in enumerate(category_test_sentence_phrases[k][i]):\n",
    "            start, end = phrase[:2]\n",
    "            print('        [{:d}] ({:d}, {:d}): {}'.format(j + 1, start, end, tokens[start:end]))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bb841-7fc0-4c85-8fdd-e0708866bce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litbank-entities",
   "language": "python",
   "name": "litbank-entities"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
